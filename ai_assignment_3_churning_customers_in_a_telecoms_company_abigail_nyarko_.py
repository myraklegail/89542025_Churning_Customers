# -*- coding: utf-8 -*-
"""AI Assignment 3: Churning Customers in a Telecoms Company_Abigail Nyarko_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JccwHVoU0937eHUUEwkDO4blzUnpQozM

**Mounting Google Drive to import the CustomerChurn Dataset and Import Python Libraries.**
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
import matplotlib.pyplot as plt
import seaborn as sns
import random as rnd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')
from sklearn.metrics import roc_curve, roc_auc_score

!pip install keras-tuner
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Input, Dense
from keras import models
from keras import Model
from tensorflow.keras import layers
import kerastuner as kt
from kerastuner.tuners import RandomSearch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
!pip install scikit-plot
import scikitplot as skplt

"""**Data Importation.**"""

customer_churn=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CustomerChurn_dataset.csv')
customer_churn

customer_churn.head() #Understanding the structure of the data

customer_churn.info()

customer_churn.dtypes

customer_churn.describe() #Summary statistics

"""Manipulating the Data"""

customer_churn = customer_churn.drop(['customerID'], axis = 1)
customer_churn.head()

customer_churn['TotalCharges'] = pd.to_numeric(customer_churn.TotalCharges, errors='coerce')
customer_churn.isnull().sum()

customer_churn[np.isnan(customer_churn['TotalCharges'])]

customer_churn[customer_churn['tenure'] == 0].index

"""

```
There are no missing values in the 'tenure'column again.
```

"""

#Removing the missing rows in the 'tenure'column since they are few
customer_churn.drop(labels=customer_churn[customer_churn['tenure'] == 0].index, axis=0, inplace=True)
customer_churn[customer_churn['tenure'] == 0].index

#Filling the missing values in 'TotalCharges'column with its mean value
customer_churn.fillna(customer_churn["TotalCharges"].mean())

#Checking for and handling missing values in the original data (Customer_Churn Dataframe)
missing_values = customer_churn.isnull().sum()
missing_values

#This replaces the "0" with a "No" and the "1" with a "Yes"
customer_churn["SeniorCitizen"]= customer_churn["SeniorCitizen"].map({0: "No", 1: "Yes"})
customer_churn.head()

print(customer_churn['Churn'].unique())

#Convert "No" and "Yes" to "0" and "1" respectively
customer_churn['Churn'] = customer_churn['Churn'].map({'Yes': 1, 'No': 0})


print(customer_churn['Churn'].unique())

numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
customer_churn[numerical_cols].describe()

#Examining the distribution of the target variable
print(customer_churn['Churn'].value_counts())



"""**Exploratory Data Analysis**"""

gender_labels = ['Male', 'Female']
churn_labels = ['No', 'Yes']

# Create subplots using 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])
fig.add_trace(go.Pie(labels=gender_labels, values=customer_churn['gender'].value_counts(), name="Gender"),
              1, 1)
fig.add_trace(go.Pie(labels=churn_labels, values=customer_churn['Churn'].value_counts(), name="Churn"),
              1, 2)

# Using `hole` to create a donut-like pie chart
fig.update_traces(hole=.4, hoverinfo="label+percent+name", textfont_size=16)

fig.update_layout(
    title_text="Churn Behaviour Across Genders",

    #Annotations to add in the center of the donut pies.
    annotations=[dict(text='Gender', x=0.16, y=0.5, font_size=20, showarrow=False),
                 dict(text='Churn', x=0.84, y=0.5, font_size=20, showarrow=False)])
fig.show()

fig = px.histogram(customer_churn, x="Churn", color="Contract", barmode="group", title="<b> Distributions of Customer Contracts<b>")
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()

color_map = {"Yes": '#00AA96', "No": '#F6C980'}
fig = px.histogram(customer_churn, x="Churn", color="SeniorCitizen", title="<b>Chrun Distribution Among Senior Citizens</b>", color_discrete_map=color_map)
fig.update_layout(width=700, height=500, bargap=0.1)
fig.show()



fig = px.histogram(customer_churn, x = 'SeniorCitizen')
fig.show()

fig = px.histogram(customer_churn, x = 'Contract')
fig.show()

fig = px.histogram(customer_churn, x = 'MonthlyCharges')
fig.show()



# Univariate analysis for a numerical feature
sns.histplot(customer_churn['SeniorCitizen'], kde=True)
plt.title('Senior Citizenship Distribution')
plt.show()

# Univariate analysis for a numerical feature
sns.histplot(customer_churn['tenure'], kde=True)
plt.title('Tenure Distribution')
plt.show()

# Univariate analysis for a numerical feature
sns.histplot(customer_churn['MonthlyCharges'], kde=True)
plt.title('Distribution for Monthly Charges')
plt.show()



# Univariate analysis for a categorical feature
sns.countplot(x='OnlineSecurity', data=customer_churn)
plt.title('Distribution for Online Security')
plt.show()

# Univariate analysis for a categorical feature
sns.countplot(x='Contract', data=customer_churn, color = 'purple')
plt.title('Contract Distribution')
plt.show()

# Univariate analysis for a categorical feature
sns.countplot(x='Dependents', data=customer_churn)
plt.title(' Distribution of Dependents')
plt.show()

# Univariate analysis for a categorical feature
sns.countplot(x='TotalCharges', data=customer_churn)
plt.title('Distribution of Total Charges')
plt.show()



# Bivariate analysis for numerical features
sns.scatterplot(x='MonthlyCharges', y='tenure', data=customer_churn)
plt.title('Scatter plot of Monthly Charges vs. Tenure')
plt.show()

# Bivariate analysis for numerical features
sns.scatterplot(x='SeniorCitizen', y='MonthlyCharges', data=customer_churn)
plt.title('Scatter plot of Senior Citizen vs. Monthly Charges')
plt.show()

# Bivariate analysis for a combination of numerical and categorical features
customer_churn['SeniorCitizen'] = customer_churn['SeniorCitizen'].astype('category')
customer_churn['SeniorCitizen'] = customer_churn['SeniorCitizen'].cat.codes

sns.boxplot(x='Churn', y='SeniorCitizen', data=customer_churn)
plt.title('Box plot of Senior Citizen by Churn')
plt.show()

# Bivariate analysis for a combination of numerical and categorical features
sns.boxplot(x='Contract', y='tenure', data=customer_churn)
plt.title('Box plot of Tenure by Contract')
plt.show()

plt.figure(figsize=(27, 12))

corr = customer_churn.apply(lambda x: pd.factorize(x)[0]).corr()
mask = np.triu(np.ones_like(corr, dtype=bool))

ax = sns.heatmap(corr, mask=mask, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, linewidths=.2, cmap='inferno', vmin=-1, vmax=1)



#Counting 1 and 0 Value in the Churn column
color = {1: "#800080", 2: "#7bc043"}

colors = customer_churn["Churn"].map(lambda x: color.get(x, "#000000"))
print(customer_churn.Churn.value_counts())
p=customer_churn.Churn.value_counts().plot(kind="bar")

# Correlation matrix
correlation_matrix = customer_churn.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""**FEATURE ENGINEERING.**

*Extracting Relevant Features that can define a customer churn.*
"""

categorical_columns = ['gender','SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
                       'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']


# Define the features/independent variables (X) and target/dependent variable (y)
X = customer_churn.drop(columns=['Churn'])  # independent variables
y = customer_churn['Churn']  # dependent variable

# Preprocess categorical features using label encoding
categorical_columns = X.select_dtypes(include=['object']).columns
label_encoder = LabelEncoder()

for col in categorical_columns:
    X[col] = label_encoder.fit_transform(X[col])

# Apply feature selection (chi-squared test) to select the top 'n' features
n =9  # Set the number of features you want to select

selector = SelectKBest(score_func=chi2, k=n)
X_new = selector.fit_transform(X, y)

# Get the selected feature indices
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_features = X.columns[selected_feature_indices]

# Print the names of the selected features
print("Selected Features:")
print(selected_features)

# Perform one-hot encoding
df = pd.get_dummies(selected_features, columns= categorical_columns)
df

#Perform one-hot encoding for all categorical columns
encoded_data = pd.get_dummies(selected_features, columns=categorical_columns, drop_first=True)

encoded_data

#Checking for and handling missing values in the encoded data
missing_values = encoded_data.isnull().sum()
missing_values

encoded_data.dtypes

selected_features

#Encoding categorical varibales using One-Hot Encoding
#identifying categorical variables
categorical_columns = [col for col in customer_churn.columns if customer_churn[col].dtype=='object']
categorical_columns

#Perform one-hot encoding for all categorical columns
encoded_data = pd.get_dummies(customer_churn, columns=categorical_columns)

encoded_data

encoded_data.dtypes

print(encoded_data.info())

#Displaying the basic statistics about the numerical columns
print(encoded_data.describe())

#Counting the unique values in each of the columns
print(encoded_data.nunique())

encoded_data

"""**MODEL TRAINING - USING FUNCTIONAL API**"""

target_column = 'Churn'
# Select the features (X) and target variable (y)
X = encoded_data
y = customer_churn[target_column]


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Split the data into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)



# Standardize the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# Building a simple functional API model using Keras
input_layer = layers.Input(shape=(X_train_scaled.shape[1],))
dense_layer = layers.Dense(64, activation='relu')(input_layer)
output_layer = layers.Dense(1, activation='sigmoid')(dense_layer)

model = keras.Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
y_pred = model.predict(X_test_scaled)
y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions
accuracy = accuracy_score(y_test, y_pred_binary)

print(f"Accuracy on the test set: {accuracy}")

"""**FINE-TUNING**"""

# Build a more complex model for fine-tuning
input_layer = layers.Input(shape=(X_train_scaled.shape[1],))
dense_layer1 = layers.Dense(128, activation='relu')(input_layer)
dropout_layer1 = layers.Dropout(0.3)(dense_layer1)
dense_layer2 = layers.Dense(64, activation='relu')(dropout_layer1)
output_layer = layers.Dense(1, activation='sigmoid')(dense_layer2)

fine_tuned_model = keras.Model(inputs=input_layer, outputs=output_layer)

# Compile the fine-tuned model
fine_tuned_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the fine-tuned model
fine_tuned_model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_data=(X_val_scaled, y_val))

## Evaluate the fine-tuned model on the validation set
val_loss, val_accuracy = fine_tuned_model.evaluate(X_val_scaled, y_val)
print(f"Validation Accuracy after Fine-Tuning: {val_accuracy}")

"""**MODEL EVALUATION.**"""

# Evaluate the model on the validation set
val_loss, val_accuracy = model.evaluate(X_val_scaled, y_val)
print(f"Validation Accuracy: {val_accuracy}")

# Evaluate the final fine-tuned model on the test set
test_loss, test_accuracy = fine_tuned_model.evaluate(X_test_scaled, y_test)
print(f"Test Accuracy after Fine-Tuning: {test_accuracy}")

# Assuming fine_tuned_model is your final fine-tuned model
predictions = fine_tuned_model.predict(X_test_scaled)

# Convert probabilities to binary predictions
y_pred_binary = (predictions > 0.5).astype(int)

# Generate confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred_binary)
print("Confusion Matrix:")
print(confusion_mat)

# Generate classification report
classification_rep = classification_report(y_test, y_pred_binary)
print("Classification Report:")
print(classification_rep)

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, predictions)

# Plot ROC curve
plt.plot(fpr, tpr, label='ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Calculate AUC
auc_score = roc_auc_score(y_test, predictions)
print(f"AUC Score: {auc_score}")

# Save the fine-tuned model
fine_tuned_model.save('fine_tuned_model.h5')